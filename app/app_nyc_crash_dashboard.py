# -*- coding: utf-8 -*-
"""FinalDashBoardDala.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UK9vFCahXrsqw1JO0YTvsdzSSdIHK_tj
"""


# app_nyc_crash_dashboard.py
import ast
import os
import re
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from sklearn.cluster import KMeans
import streamlit as st

# -----------------------------
# Streamlit Page Settings
# -----------------------------
st.set_page_config(page_title="NYC Crash Dashboard", layout="wide")

PARQUET_PATH = "nyc_crashes.parquet"

@st.cache_data(show_spinner="Loading datasetâ€¦")
def load_data():
     # Parquet-only loader: read `nyc_crashes.parquet` if present. Return empty DataFrame on any failure.
     if os.path.exists(PARQUET_PATH):
          try:
               df = pd.read_parquet(PARQUET_PATH)
               st.info(f"Loaded {len(df):,} rows from Parquet file.")
               return df
          except Exception as e:
               st.error(f"Failed to read Parquet `{PARQUET_PATH}`: {e}")
               return pd.DataFrame()

     st.error(f"Parquet dataset `{PARQUET_PATH}` not found in the repository.")
     return pd.DataFrame()

# Do NOT load data at import time on the server. Start with an empty DataFrame
# and load on-demand via the Streamlit UI to avoid import-time crashes on Cloud.
df = pd.DataFrame()

# Data preparation is skipped at import time. When data is present we perform
# normalization; otherwise we initialize safe defaults so the UI can render.
borough_mapping = {
     'MANHATTAN': 'Manhattan',
     'BROOKLYN': 'Brooklyn',
     'QUEENS': 'Queens',
     'BRONX': 'Bronx',
     'STATEN ISLAND': 'Staten Island'
}

if not df.empty:
     # Normalize and cast useful columns
     if "BOROUGH" in df.columns:
          df["BOROUGH"] = df["BOROUGH"].str.title().replace(borough_mapping)
          df["BOROUGH"] = df["BOROUGH"].fillna("Unknown")
     else:
          df["BOROUGH"] = "Unknown"

     # Convert crash datetime to datetime (coerce errors)
     df["CRASH_DATETIME"] = pd.to_datetime(df.get("CRASH_DATETIME", pd.NaT), errors="coerce")
     # YEAR for slider and groupings
     df["YEAR"] = df["CRASH_DATETIME"].dt.year
     df["MONTH"] = df["CRASH_DATETIME"].dt.month
     df["HOUR"] = df["CRASH_DATETIME"].dt.hour
     df["DAY_OF_WEEK"] = df["CRASH_DATETIME"].dt.day_name()

     # Cast numeric injury/killed counts to numeric (safe)
     num_cols = [
          "NUMBER OF PERSONS INJURED", "NUMBER OF PERSONS KILLED",
          "NUMBER OF PEDESTRIANS INJURED", "NUMBER OF PEDESTRIANS KILLED",
          "NUMBER OF CYCLIST INJURED", "NUMBER OF CYCLIST KILLED",
          "NUMBER OF MOTORIST INJURED", "NUMBER OF MOTORIST KILLED"
     ]
     for c in num_cols:
          if c in df.columns:
               df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0).astype(int)
          else:
               df[c] = 0

     # Helpful aggregated numeric columns
     df["TOTAL_INJURED"] = df[["NUMBER OF PERSONS INJURED",
                                     "NUMBER OF PEDESTRIANS INJURED",
                                     "NUMBER OF CYCLIST INJURED",
                                     "NUMBER OF MOTORIST INJURED"]].sum(axis=1)
     df["TOTAL_KILLED"] = df[["NUMBER OF PERSONS KILLED",
                                    "NUMBER OF PEDESTRIANS KILLED",
                                    "NUMBER OF CYCLIST KILLED",
                                    "NUMBER OF MOTORIST KILLED"]].sum(axis=1)

     # Create severity score for advanced analysis
     df["SEVERITY_SCORE"] = (df["TOTAL_INJURED"] * 1 + df["TOTAL_KILLED"] * 5)

     # FULL_ADDRESS fallback
     if "FULL ADDRESS" not in df.columns:
          df["FULL ADDRESS"] = df.get("ON STREET NAME", "").fillna("") + ", " + df.get("BOROUGH", "")

     # Latitude / Longitude as numeric
     for coord in ("LATITUDE", "LONGITUDE"):
          if coord in df.columns:
               df[coord] = pd.to_numeric(df[coord], errors="coerce")
          else:
               df[coord] = np.nan
else:
     # Safe defaults when no data is loaded yet
     df["BOROUGH"] = pd.Series([], dtype=object)
     df["YEAR"] = pd.Series([], dtype="float64")

# Parse ALL_VEHICLE_TYPES (which may be a string representation of a list) and create a flattened column
def parse_vehicle_list(v):
     if pd.isna(v):
          return []
     # If it's already a Python list object (rare in CSV), handle
     if isinstance(v, list):
          return [str(x).strip() for x in v if str(x).strip()]
     s = str(v).strip()
     # Try literal_eval if it's like "['SUV/Station Wagon', 'Sedan']"
     try:
          parsed = ast.literal_eval(s)
          if isinstance(parsed, (list, tuple)):
               return [str(x).strip() for x in parsed if str(x).strip()]
     except Exception:
          # fallback: comma-separated
          parts = [p.strip() for p in s.split(",") if p.strip()]
          return parts
     return []

df["VEHICLE_TYPES_LIST"] = df.get("ALL_VEHICLE_TYPES", "").apply(parse_vehicle_list)

# Expand vehicle types per row into a flat list column for easier counting
all_vehicle_types_flat = [vt for sub in df["VEHICLE_TYPES_LIST"] for vt in sub]
vehicle_type_counts = pd.Series(all_vehicle_types_flat).value_counts()
# Top 10 vehicle types for charts / heatmap combos
TOP_VEHICLE_TYPES = vehicle_type_counts.head(10).index.tolist()

# Parse contributing factors (all contributing factors column may be a list or string)
def parse_factor_list(v):
     if pd.isna(v):
          return []
     if isinstance(v, list):
          return [str(x).strip() for x in v if str(x).strip()]
     s = str(v).strip()
     try:
          parsed = ast.literal_eval(s)
          if isinstance(parsed, (list, tuple)):
               return [str(x).strip() for x in parsed if str(x).strip()]
     except Exception:
          parts = [p.strip() for p in s.split(",") if p.strip()]
          return parts
     return []

# Try to handle both ALL_CONTRIBUTING_FACTORS and ALL_CONTRIBUTING_FACTORS_STR
if "ALL_CONTRIBUTING_FACTORS" in df.columns:
     df["FACTORS_LIST"] = df["ALL_CONTRIBUTING_FACTORS"].apply(parse_factor_list)
elif "ALL_CONTRIBUTING_FACTORS_STR" in df.columns:
     df["FACTORS_LIST"] = df["ALL_CONTRIBUTING_FACTORS_STR"].apply(parse_factor_list)
else:
     # fallback to specific columns if provided
     parts = []
     for i in range(1, 4):
          c = f"CONTRIBUTING FACTOR VEHICLE {i}"
          if c in df.columns:
               parts.append(df[c].fillna("").astype(str))
     if parts:
          df["FACTORS_LIST"] = (pd.Series([";".join(x) for x in zip(*parts)]) if parts else pd.Series([[]]*len(df))).apply(parse_factor_list)
     else:
          df["FACTORS_LIST"] = [[] for _ in range(len(df))]

all_factors_flat = [f for sub in df["FACTORS_LIST"] for f in sub]
factor_counts = pd.Series(all_factors_flat).value_counts()
TOP_FACTORS = factor_counts.head(10).index.tolist()

# PERSON_TYPE (type of persons involved)
# ensure PERSON_TYPE column exists
if "PERSON_TYPE" not in df.columns:
     df["PERSON_TYPE"] = df.get("PERSON_TYPE", "Unknown").fillna("Unknown")

# POSITION_IN_VEHICLE_CLEAN is provided in dataset per your list, ensure it's present
if "POSITION_IN_VEHICLE_CLEAN" not in df.columns:
     df["POSITION_IN_VEHICLE_CLEAN"] = df.get("POSITION_IN_VEHICLE_CLEAN", "").fillna("Unknown")

# Ensure other person-related columns exist (for new plots)
for col in ["PERSON_AGE", "PERSON_SEX", "BODILY_INJURY", "SAFETY_EQUIPMENT", "EMOTIONAL_STATUS", "UNIQUE_ID", "EJECTION", "ZIP CODE", "PERSON_INJURY"]:
     if col not in df.columns:
          # Create a placeholder column if not found (assuming person-level data is in the merged set)
          if col == "UNIQUE_ID":
               df[col] = df.index + 1
          elif col == "PERSON_AGE":
               df[col] = pd.to_numeric(df.get(col, np.nan), errors='coerce').fillna(0).astype(int) # Coerce age to int, fill missing/bad with 0
          elif col in ["EJECTION", "ZIP CODE", "PERSON_INJURY"]:
               df[col] = df.get(col, "Unknown").fillna("Unknown")
          else:
               df[col] = df.get(col, "Unknown").fillna("Unknown")

# Ensure additional columns exist
for col in ["COMPLAINT", "VEHICLE TYPE CODE 1", "CONTRIBUTING FACTOR VEHICLE 1"]:
     if col not in df.columns:
          df[col] = "Unknown"

# Small helper to add jitter to lat/lon to separate overlapping points
def jitter_coords(series, scale=0.0006):
     # scale tuned for city-level jitter
     return series + np.random.normal(loc=0, scale=scale, size=series.shape)

# Define consistent borough colors with proper capitalization
BOROUGH_COLORS = {
     'Manhattan': '#2ECC71',  # Green
     'Brooklyn': '#E74C3C',   # Red
     'Queens': '#3498DB',     # Blue
     'Bronx': '#F39C12',      # Orange
     'Staten Island': '#9B59B6', # Purple
     'Unknown': '#95A5A6'     # Gray
}

# Year bounds used for UI controls â€” robustly derive from CRASH_DATETIME if YEAR missing
if "YEAR" not in df.columns or df["YEAR"].isna().all():
     if "CRASH_DATETIME" in df.columns:
          df["CRASH_DATETIME"] = pd.to_datetime(df["CRASH_DATETIME"], errors="coerce")
          df["YEAR"] = df["CRASH_DATETIME"].dt.year
     else:
          # no crash datetime available; create empty YEAR column
          df["YEAR"] = pd.Series([pd.NA] * len(df))

if not df["YEAR"].isna().all():
     min_year = int(df["YEAR"].min())
     max_year = int(df["YEAR"].max())
else:
     # fallback to reasonable defaults
     min_year = 2010
     max_year = pd.Timestamp.now().year

# ------------------------------------------------------------------
# Create Enhanced Dash app layout
# ------------------------------------------------------------------
# Dash layout removed â€” converted to Streamlit below

# -------------------------
# Helper: parse search query - SIMPLIFIED VERSION (no contributing factors in search)
# -------------------------
def parse_search_query(q):
    q = (q or "").lower().strip()
    found = {}

    # Extract years and year ranges
    year_pattern = r'\b(20\d{2})\b'
    years_found = re.findall(year_pattern, q)
    if years_found:
        years = sorted([int(y) for y in years_found])
        if len(years) >= 2:
            found["year_range"] = [years[0], years[-1]]
        else:
            found["year"] = years[0]

    # Borough detection
    borough_keywords = {
        'manhattan': 'Manhattan',
        'brooklyn': 'Brooklyn',
        'queens': 'Queens',
        'bronx': 'Bronx',
        'staten': 'Staten Island',
        'staten island': 'Staten Island'
    }
    for keyword, borough in borough_keywords.items():
        if keyword in q:
            found["borough"] = [borough]
            break

    # Vehicle type detection
    vehicle_keywords = {
        'suv': 'SUV/Station Wagon',
        'station wagon': 'SUV/Station Wagon',
        'sedan': 'Sedan',
        'bicycle': 'Bicycle',
        'bike': 'Bicycle',
        'ambulance': 'Ambulance',
        'bus': 'Bus',
        'motorcycle': 'Motorcycle',
        'pickup': 'Pickup Truck',
        'pickup truck': 'Pickup Truck',
        'taxi': 'Taxi',
        'truck': 'Truck/Commercial',
        'commercial': 'Truck/Commercial',
        'van': 'Van',
        'pedicab': 'Pedicab'
    }
    vehicle_matches = []
    for keyword, vehicle_type in vehicle_keywords.items():
        if keyword in q:
            vehicle_matches.append(vehicle_type)
    if vehicle_matches:
        found["vehicle"] = vehicle_matches

    # Person type and demographics
    person_type_matches = []
    if 'pedestrian' in q:
        person_type_matches.append('Pedestrian')
    if 'cyclist' in q or 'bicyclist' in q:
        person_type_matches.append('Bicyclist')
    if 'motorist' in q:
        person_type_matches.append('Motorist')
    if 'driver' in q:
        person_type_matches.append('Driver')
    if 'occupant' in q:
        person_type_matches.append('Occupant')
    if person_type_matches:
        found["person_type"] = person_type_matches

    # Gender detection
    if 'female' in q or 'woman' in q or 'women' in q:
        found["gender"] = ['F']
    elif 'male' in q or 'man' in q or 'men' in q:
        found["gender"] = ['M']

    # Direct F/M detection
    if ' f ' in f" {q} " or q.endswith(' f') or q.startswith('f '):
        found["gender"] = ['F']
    elif ' m ' in f" {q} " or q.endswith(' m') or q.startswith('m '):
        found["gender"] = ['M']

    # Injury severity
    injury_matches = []
    if 'injured' in q or 'injury' in q:
        injury_matches.append('Injured')
    if 'killed' in q or 'fatal' in q or 'fatality' in q or 'death' in q or 'died' in q:
        injury_matches.append('Killed')
    if 'unspecified' in q:
        injury_matches.append('Unspecified')
    if injury_matches:
        found["injury"] = injury_matches

    return found

# -------------------------
# Compute figures (converted from Dash callback)
# -------------------------
@st.cache_data(ttl=600)
def compute_figures(year_range=None, boroughs=None, vehicles=None, factors=None, injuries=None, person_type=None, search_text=None):
     # Wrapper of previous Dash callback logic â€” compute all figures for given filters
     # Provide defaults
     if year_range is None:
          year_range = [int(min_year), int(max_year)]

     dff = df.copy()

     # Ensure YEAR exists on the working copy (derive from CRASH_DATETIME if needed)
     if "YEAR" not in dff.columns or dff["YEAR"].isna().all():
          if "CRASH_DATETIME" in dff.columns:
               dff["CRASH_DATETIME"] = pd.to_datetime(dff["CRASH_DATETIME"], errors="coerce")
               dff["YEAR"] = dff["CRASH_DATETIME"].dt.year
          else:
               dff["YEAR"] = pd.Series([pd.NA] * len(dff))

     # --- Apply search query ---
     if search_text:
          parsed = parse_search_query(search_text)
          print(f"Parsed search: {parsed}")  # Debug line

          # Year range from search
          if "year_range" in parsed:
               yr_range = parsed["year_range"]
               year_range = [max(year_range[0], yr_range[0]), min(year_range[1], yr_range[1])]
          elif "year" in parsed:
               yr = parsed["year"]
               year_range = [max(year_range[0], yr), min(year_range[1], yr)]

          # Borough filter from search
          if "borough" in parsed:
               if boroughs:
                    boroughs = list(set(boroughs) & set(parsed["borough"]))
               else:
                    boroughs = parsed["borough"]

          # Vehicle filter from search
          if "vehicle" in parsed:
               if vehicles:
                    vehicles = list(set(vehicles) & set(parsed["vehicle"]))
               else:
                    vehicles = parsed["vehicle"]

          # Person type filter from search
          if "person_type" in parsed:
               if person_type:
                    person_type = list(set(person_type) & set(parsed["person_type"]))
               else:
                    person_type = parsed["person_type"]

          # Injury filter from search
          if "injury" in parsed:
               if injuries:
                    injuries = list(set(injuries) & set(parsed["injury"]))
               else:
                    injuries = parsed["injury"]

          # Gender filter from search
          if "gender" in parsed:
               gender_filter = parsed["gender"]
               dff = dff[dff["PERSON_SEX"].isin(gender_filter)]

     # --- Apply year range ---
     if year_range and len(year_range) == 2:
          y0, y1 = int(year_range[0]), int(year_range[1])
          dff = dff[(dff["YEAR"] >= y0) & (dff["YEAR"] <= y1)]

     # --- Apply borough filter ---
     if boroughs:
          dff = dff[dff["BOROUGH"].isin(boroughs)]

     # --- Apply injury filter ---
     if injuries:
          dff = dff[dff["PERSON_INJURY"].fillna("").astype(str).isin([str(i) for i in injuries])]

     # --- Apply vehicle filter ---
     if vehicles:
          mask = dff["VEHICLE_TYPES_LIST"].apply(lambda lst: any(v in (lst if isinstance(lst, list) else []) for v in vehicles))
          dff = dff[mask]

     # --- Apply contributing factor filter ---
     if factors:
          # Clean the factors to remove any extra characters
          clean_factors = [str(f).strip().strip("[]'\"") for f in factors]
          mask = dff["FACTORS_LIST"].apply(lambda lst: any(
              any(clean_f in str(fact).strip().strip("[]'\"") for clean_f in clean_factors)
              for fact in (lst if isinstance(lst, list) else [])
          ))
          dff = dff[mask]

     # --- Apply person type filter ---
     if person_type:
          dff = dff[dff["PERSON_TYPE"].isin(person_type)]

     # Calculate statistics for live stats
     total_crashes = len(dff)
     total_injuries = dff["TOTAL_INJURED"].sum()
     total_killed = dff["TOTAL_KILLED"].sum()
     avg_injuries_per_crash = total_injuries / total_crashes if total_crashes > 0 else 0

     # Define pink plot template
     pink_template = {
         'layout': {
             'paper_bgcolor': '#FFE6E6',
             'plot_bgcolor': '#FFE6E6',
             'font': {'color': '#2C3E50'},
             'xaxis': {'gridcolor': '#FFB6C1', 'linecolor': '#2C3E50'},
             'yaxis': {'gridcolor': '#FFB6C1', 'linecolor': '#2C3E50'}
         }
     }

     # Performance tuning: sampling limits for heavy plots (reduced for responsiveness)
     MAX_MAP_POINTS = 2000
     MAX_DENSITY_POINTS = 5000
     MAX_KMEANS_SAMPLE = 1000
     MAX_VEHICLE_LISTS = 10000

     # Vibrant color sequence for line charts
     vibrant_colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD', '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9']

     # ---------- LAMA'S FIGURES ----------
     # 1) Injuries by Borough - Consistent borough colors
     injuries_by_borough = dff.groupby("BOROUGH")["TOTAL_INJURED"].sum().reset_index().sort_values("TOTAL_INJURED", ascending=False)
     fig_inj_borough = px.bar(injuries_by_borough, x="BOROUGH", y="TOTAL_INJURED",
                              labels={"TOTAL_INJURED": "Total Injured", "BOROUGH": "Borough"},
                              text="TOTAL_INJURED",
                              color="BOROUGH",
                              color_discrete_map=BOROUGH_COLORS)
     fig_inj_borough.update_traces(textposition="outside")
     fig_inj_borough.update_layout(margin=dict(t=40, b=20), template=pink_template, showlegend=False)

     # 2) Crashes by Contributing Factor - Purple shades
     factor_rows = []
     for _, row in dff.iterrows():
          for f in row["FACTORS_LIST"]:
               factor_rows.append((f, row["UNIQUE_ID"] if "UNIQUE_ID" in row else 1))
     factor_df = pd.DataFrame(factor_rows, columns=["Factor", "UID"]) if factor_rows else pd.DataFrame(columns=["Factor", "UID"])
     factor_counts_df = factor_df["Factor"].value_counts().head(15).reset_index()
     factor_counts_df.columns = ["Factor", "Count"]
     fig_factor = px.bar(factor_counts_df, x="Count", y="Factor", orientation="h",
                         labels={"Count": "Number of Crashes", "Factor": "Contributing Factor"},
                         color="Count",
                         color_continuous_scale="purples")
     fig_factor.update_layout(margin=dict(t=40, b=20), yaxis={'categoryorder':'total ascending'}, template=pink_template)

     # 3) Crashes per Year (line) by Borough - Consistent borough colors
     year_group = dff.groupby(["YEAR", "BOROUGH"]).size().reset_index(name="Crashes")
     if not year_group.empty:
          fig_year = px.line(year_group, x="YEAR", y="Crashes", color="BOROUGH", markers=True,
                           color_discrete_map=BOROUGH_COLORS)
          fig_year.update_layout(template=pink_template)
     else:
          fig_year = go.Figure()
          fig_year.update_layout(title="Crashes per Year (no data for selection)")

     # 4) Crash locations points map - Consistent borough colors
     df_map = dff.dropna(subset=["LATITUDE", "LONGITUDE"]).copy()
     if not df_map.empty:
          # sample for plotting to keep browser responsive
          if len(df_map) > MAX_MAP_POINTS:
               df_map_plot = df_map.sample(n=MAX_MAP_POINTS, random_state=42)
          else:
               df_map_plot = df_map
          df_map_plot["_LAT_JIT"] = jitter_coords(df_map_plot["LATITUDE"].fillna(0).astype(float), scale=0.0005)
          df_map_plot["_LON_JIT"] = jitter_coords(df_map_plot["LONGITUDE"].fillna(0).astype(float), scale=0.0005)
          fig_map = px.scatter_mapbox(df_map_plot, lat="_LAT_JIT", lon="_LON_JIT", color="BOROUGH",
                                       hover_name="FULL ADDRESS",
                                       hover_data={"FULL ADDRESS": True,
                                                   "TOTAL_INJURED": True,
                                                   "TOTAL_KILLED": True,
                                                   "CRASH_DATETIME": True,
                                                   "_LAT_JIT": False, "_LON_JIT": False},
                                       zoom=9, height=500,
                                       mapbox_style="open-street-map",
                                       color_discrete_map=BOROUGH_COLORS)
          fig_map.update_traces(marker=dict(size=8, opacity=0.7))
          fig_map.update_layout(margin=dict(t=0), template=pink_template)
     else:
          fig_map = go.Figure()
          fig_map.update_layout(title="No location data to display")

     # 5) Gender Distribution
     gender_dist = dff.groupby("PERSON_SEX")["UNIQUE_ID"].count().reset_index(name="Count")
     fig_gender = px.pie(gender_dist, names="PERSON_SEX", values="Count",
                        color_discrete_sequence=['#4A90E2', '#FF8DA1', '#95A5A6'])
     fig_gender.update_layout(margin=dict(t=40, b=20), template=pink_template)

     # 6) Safety Equipment Usage (Top 5)
     safety_dist = dff.groupby("SAFETY_EQUIPMENT")["UNIQUE_ID"].count().reset_index(name="Count")
     safety_dist = safety_dist.sort_values("Count", ascending=False).head(5)
     fig_safety = px.pie(safety_dist, names="SAFETY_EQUIPMENT", values="Count",
                         labels={"SAFETY_EQUIPMENT": "Safety Equipment", "Count": "Number of Records"},
                         color_discrete_sequence=['#FF8DA1', '#FFB6C1', '#FFD1DC', '#FFAEC9', '#FF85A1'])
     fig_safety.update_layout(margin=dict(t=40, b=20), template=pink_template)

     # 7) Emotional State Distribution - Pink color
     emotional_dist = dff.groupby("EMOTIONAL_STATUS")["UNIQUE_ID"].count().reset_index(name="Count")
     emotional_dist = emotional_dist.sort_values("Count", ascending=False)
     fig_emotional = px.bar(emotional_dist, x="EMOTIONAL_STATUS", y="Count",
                             labels={"EMOTIONAL_STATUS": "Emotional State", "Count": "Number of Records"},
                             color_discrete_sequence=['#FF8DA1'])
     fig_emotional.update_layout(margin=dict(t=40, b=20), xaxis={'categoryorder':'total descending'}, template=pink_template, showlegend=False)

     # 8) Age Distribution with Marginal Box Plot
     dff["PERSON_AGE"] = pd.to_numeric(dff["PERSON_AGE"], errors='coerce')
     fig_age_hist = px.histogram(dff, x="PERSON_AGE", nbins=30,
                           marginal="box",
                           hover_data=["PERSON_AGE"],
                           color_discrete_sequence=['#FF6B6B'])
     fig_age_hist.update_layout(
         margin=dict(t=40, b=20),
         xaxis_title="Age",
         yaxis_title="Count",
         template=pink_template
     )

     # 9) Injuries by Person Type Over Time
     person_type_time = dff.groupby(["YEAR", "PERSON_TYPE"]).agg({
         "TOTAL_INJURED": "sum"
     }).reset_index()
     fig_person_time = px.bar(person_type_time, x="YEAR", y="TOTAL_INJURED", color="PERSON_TYPE",
                             barmode="stack",
                             color_discrete_sequence=vibrant_colors)
     fig_person_time.update_layout(
     margin=dict(t=40, b=20),
          legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01),
          template=pink_template
     )

     # ---------- ADDITIONAL FIGURES (derived from available columns) ----------
     # Top ZIP codes by crash count
     if 'ZIP CODE' in dff.columns:
          zip_counts = dff['ZIP CODE'].fillna('Unknown').value_counts().nlargest(15).reset_index()
          zip_counts.columns = ['ZIP', 'Count']
          fig_zip = px.bar(zip_counts, x='ZIP', y='Count', labels={'ZIP': 'ZIP Code', 'Count': 'Crashes'}, color='Count', color_continuous_scale='Blues')
          fig_zip.update_layout(template=pink_template)
     else:
          fig_zip = go.Figure(); fig_zip.update_layout(title='No ZIP code data available')

     # Top streets (ON STREET NAME)
     if 'ON STREET NAME' in dff.columns:
          street_counts = dff['ON STREET NAME'].fillna('Unknown').value_counts().nlargest(20).reset_index()
          street_counts.columns = ['Street', 'Count']
          fig_top_streets = px.bar(street_counts, x='Count', y='Street', orientation='h', labels={'Street': 'On Street', 'Count': 'Crashes'}, color='Count', color_continuous_scale='oranges')
          fig_top_streets.update_layout(template=pink_template)
     else:
          fig_top_streets = go.Figure(); fig_top_streets.update_layout(title='No street data available')

     # Top intersections (ON STREET NAME + CROSS STREET NAME)
     if 'ON STREET NAME' in dff.columns and 'CROSS STREET NAME' in dff.columns:
          inter_combo = (dff['ON STREET NAME'].fillna('') + ' & ' + dff['CROSS STREET NAME'].fillna('')).str.strip()
          inter_counts = inter_combo.value_counts().nlargest(20).reset_index()
          inter_counts.columns = ['Intersection', 'Count']
          fig_top_intersections = px.bar(inter_counts, x='Count', y='Intersection', orientation='h', labels={'Intersection': 'Intersection', 'Count': 'Crashes'}, color='Count', color_continuous_scale='teal')
          fig_top_intersections.update_layout(template=pink_template)
     else:
          fig_top_intersections = go.Figure(); fig_top_intersections.update_layout(title='No intersection data available')

     # Vehicle co-occurrence heatmap from ALL_VEHICLE_TYPES (pairwise counts)
     if 'ALL_VEHICLE_TYPES' in dff.columns:
          # Build co-occurrence matrix for top N vehicle types
          vehicle_lists = dff['ALL_VEHICLE_TYPES'].apply(parse_vehicle_list)
          # sample vehicle lists if too many rows to speed up processing
          if len(vehicle_lists) > MAX_VEHICLE_LISTS:
               vehicle_lists = vehicle_lists.sample(n=MAX_VEHICLE_LISTS, random_state=42)
          flat = [v for sub in vehicle_lists for v in sub]
          top_vts = pd.Series(flat).value_counts().head(12).index.tolist()
          # Build matrix
          matrix = pd.DataFrame(0, index=top_vts, columns=top_vts)
          for lst in vehicle_lists:
               for a in lst:
                    for b in lst:
                         if a in top_vts and b in top_vts:
                              matrix.at[a, b] += 1
          fig_vehicle_cooccurrence = px.imshow(matrix, labels=dict(x='Vehicle Type', y='Vehicle Type', color='Co-occurrence'), x=matrix.columns, y=matrix.index, color_continuous_scale='Viridis')
          fig_vehicle_cooccurrence.update_layout(template=pink_template)
     else:
          fig_vehicle_cooccurrence = go.Figure(); fig_vehicle_cooccurrence.update_layout(title='No vehicle-type co-occurrence available')

     # Injury counts breakdown (individual injury columns)
     injury_columns = [
          'NUMBER OF PERSONS INJURED', 'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF MOTORIST INJURED'
     ]
     existing_injury_cols = [c for c in injury_columns if c in dff.columns]
     if existing_injury_cols:
          inj_df = dff[existing_injury_cols].sum().reset_index()
          inj_df.columns = ['Injury Type', 'Count']
          fig_injury_counts = px.bar(inj_df, x='Injury Type', y='Count', color='Count', color_continuous_scale='reds')
          fig_injury_counts.update_layout(template=pink_template)
     else:
          fig_injury_counts = go.Figure(); fig_injury_counts.update_layout(title='No individual injury counts available')

     # Contributing factor vehicle 2 and 3
     if 'CONTRIBUTING FACTOR VEHICLE 2' in dff.columns:
          f2 = dff['CONTRIBUTING FACTOR VEHICLE 2'].fillna('Unknown').value_counts().nlargest(15).reset_index()
          f2.columns = ['Factor', 'Count']
          fig_factor2 = px.bar(f2, x='Count', y='Factor', orientation='h', color='Count', color_continuous_scale='purples')
          fig_factor2.update_layout(template=pink_template)
     else:
          fig_factor2 = go.Figure(); fig_factor2.update_layout(title='No factor 2 data')

     if 'CONTRIBUTING FACTOR VEHICLE 3' in dff.columns:
          f3 = dff['CONTRIBUTING FACTOR VEHICLE 3'].fillna('Unknown').value_counts().nlargest(15).reset_index()
          f3.columns = ['Factor', 'Count']
          fig_factor3 = px.bar(f3, x='Count', y='Factor', orientation='h', color='Count', color_continuous_scale='purples')
          fig_factor3.update_layout(template=pink_template)
     else:
          fig_factor3 = go.Figure(); fig_factor3.update_layout(title='No factor 3 data')

     # Pedestrian role distribution (PED_ROLE) if present
     if 'PED_ROLE' in dff.columns:
          ped_counts = dff['PED_ROLE'].fillna('Unknown').value_counts().reset_index()
          ped_counts.columns = ['Ped Role', 'Count']
          fig_ped_role = px.pie(ped_counts, names='Ped Role', values='Count')
          fig_ped_role.update_layout(template=pink_template)
     else:
          fig_ped_role = go.Figure(); fig_ped_role.update_layout(title='No pedestrian role data')

     # Daily crash trend (by date)
     if 'CRASH_DATETIME' in dff.columns:
          dff['CRASH_DATE'] = pd.to_datetime(dff['CRASH_DATETIME'], errors='coerce').dt.date
          daily = dff.groupby('CRASH_DATE').size().reset_index(name='Count')
          daily = daily.sort_values('CRASH_DATE')
          fig_daily_trend = px.line(daily, x='CRASH_DATE', y='Count', labels={'CRASH_DATE': 'Date', 'Count': 'Crashes'})
          fig_daily_trend.update_layout(template=pink_template)
     else:
          fig_daily_trend = go.Figure(); fig_daily_trend.update_layout(title='No crash datetime for daily trend')

     # --- Summary text generation ---
     summary = f"ðŸ“Š Currently showing: {total_crashes:,} crashes | {total_injuries:,} injured | {total_killed:,} fatalities"

     # ---------- DAREEN'S FIGURES ----------
     # 1. Borough chart - Consistent borough colors
     borough_df = dff.groupby("BOROUGH").size().reset_index(name="Count").sort_values("Count", ascending=False)
     fig_borough_dareen = px.bar(borough_df, x="BOROUGH", y="Count",
                                color="BOROUGH",
                                color_discrete_map=BOROUGH_COLORS)
     fig_borough_dareen.update_layout(template=pink_template, showlegend=False)

     # 2. Injury-Type chart (horizontal) - Teal color
     injury_df = dff.groupby("BODILY_INJURY").size().reset_index(name="Count").sort_values("Count", ascending=False)
     fig_injury_dareen = px.bar(
          injury_df,
          x="Count",
          y="BODILY_INJURY",
          orientation="h",
          labels={"BODILY_INJURY": "Bodily Injury", "Count": "Number of Cases"},
          color_discrete_sequence=['#20B2AA']  # Teal color
     )
     fig_injury_dareen.update_yaxes(categoryorder="total ascending")
     fig_injury_dareen.update_layout(margin=dict(t=40, b=20), template=pink_template, showlegend=False)

     # 3. Ejection chart (grouped by person type)
     fig_ejection = px.bar(
          dff.groupby(["PERSON_TYPE", "EJECTION"]).size().reset_index(name="Count"),
          x="EJECTION",
          y="Count",
          color="PERSON_TYPE",
          labels={"EJECTION": "Ejection Status", "Count": "Number of Cases"},
          color_discrete_sequence=vibrant_colors
     )
     fig_ejection.update_layout(template=pink_template)

     # 4. Complaint chart (grouped by person type)
     top_complaints = dff["COMPLAINT"].value_counts().nlargest(10).index
     fig_complaint = px.bar(
          dff[dff["COMPLAINT"].isin(top_complaints)].groupby(["COMPLAINT", "PERSON_TYPE"])
          .size().reset_index(name="Count"),
          x="COMPLAINT",
          y="Count",
          color="PERSON_TYPE",
          labels={"COMPLAINT": "Complaint Type", "Count": "Number of Cases"},
          color_discrete_sequence=vibrant_colors
     )
     fig_complaint.update_layout(xaxis_tickangle=-45, template=pink_template)

     # 5. Vehicle Factor Heatmap
     top_factors = dff["CONTRIBUTING FACTOR VEHICLE 1"].value_counts().nlargest(10).index
     fig_vehicle_factor = px.density_heatmap(
          dff[dff["CONTRIBUTING FACTOR VEHICLE 1"].isin(top_factors)],
          x="VEHICLE TYPE CODE 1",
          y="CONTRIBUTING FACTOR VEHICLE 1",
          labels={"VEHICLE TYPE CODE 1": "Vehicle Type", "CONTRIBUTING FACTOR VEHICLE 1": "Contributing Factor"},
          color_continuous_scale="viridis"
     )
     fig_vehicle_factor.update_layout(template=pink_template)

     # 6. Position chart
     fig_position = px.bar(
          dff.groupby(["POSITION_IN_VEHICLE_CLEAN", "PERSON_INJURY"]).size().reset_index(name="Count"),
          x="POSITION_IN_VEHICLE_CLEAN",
          y="Count",
          color="PERSON_INJURY",
          labels={"POSITION_IN_VEHICLE_CLEAN": "Position in Vehicle", "Count": "Number of Cases"},
          color_discrete_sequence=vibrant_colors
     )
     fig_position.update_layout(template=pink_template)

     # 7. Vehicle trend chart - Vibrant colors
     trend_df = dff.groupby(["YEAR", "VEHICLE TYPE CODE 1"]).size().reset_index(name="Count")
     fig_vehicle_trend = px.line(
          trend_df,
          x="YEAR",
          y="Count",
          color="VEHICLE TYPE CODE 1",
          labels={"YEAR": "Year", "Count": "Number of Crashes", "VEHICLE TYPE CODE 1": "Vehicle Type"},
          color_discrete_sequence=vibrant_colors
     )
     fig_vehicle_trend.update_layout(template=pink_template)

     # ---------- ADVANCED ANALYTICS FIGURES ----------
     # 1. Hotspot Cluster Map
     df_coords = dff.dropna(subset=["LATITUDE", "LONGITUDE"]).copy()
     if len(df_coords) > 10:
          # sample for fitting kmeans, then predict clusters for full dataset
          sample_n = min(MAX_KMEANS_SAMPLE, len(df_coords))
          kmeans_sample = df_coords.sample(n=sample_n, random_state=42)
          coords_sample = kmeans_sample[["LATITUDE", "LONGITUDE"]].values
          kmeans = KMeans(n_clusters=min(10, len(kmeans_sample)), random_state=42)
          kmeans.fit(coords_sample)
          df_coords["CLUSTER"] = kmeans.predict(df_coords[["LATITUDE", "LONGITUDE"]].values)
          cluster_sizes = df_coords.groupby("CLUSTER").size()
          df_coords["CLUSTER_SIZE"] = df_coords["CLUSTER"].map(cluster_sizes)

          # sample for plotting map points to keep browser responsive
          if len(df_coords) > MAX_MAP_POINTS:
               df_coords_plot = df_coords.sample(n=MAX_MAP_POINTS, random_state=42)
          else:
               df_coords_plot = df_coords

          fig_hotspot = px.scatter_mapbox(df_coords_plot, lat="LATITUDE", lon="LONGITUDE",
                                         color="CLUSTER_SIZE", size="CLUSTER_SIZE",
                                         hover_name="FULL ADDRESS",
                                         hover_data={"CLUSTER_SIZE": True, "TOTAL_INJURED": True},
                                         zoom=9, height=500,
                                         mapbox_style="open-street-map",
                                         color_continuous_scale="viridis")
          fig_hotspot.update_layout(template=pink_template)
     else:
          fig_hotspot = go.Figure()
          fig_hotspot.update_layout(title="Not enough location data for clustering")

     # 2. Correlation Heatmap
     numeric_cols = ['PERSON_AGE', 'TOTAL_INJURED', 'TOTAL_KILLED', 'SEVERITY_SCORE', 'HOUR']
     available_numeric = [col for col in numeric_cols if col in dff.columns]

     if len(available_numeric) > 1:
          corr_matrix = dff[available_numeric].corr()
          fig_corr = ff.create_annotated_heatmap(
               z=corr_matrix.values,
               x=corr_matrix.columns.tolist(),
               y=corr_matrix.columns.tolist(),
               annotation_text=corr_matrix.round(2).values,
               colorscale='Blues'
          )
          fig_corr.update_layout(template=pink_template)
     else:
          fig_corr = go.Figure()
          fig_corr.update_layout(title="Not enough numeric data for correlation analysis")

     # 3. Temporal Patterns
     if 'HOUR' in dff.columns and 'DAY_OF_WEEK' in dff.columns:
          temporal_data = dff.groupby(['DAY_OF_WEEK', 'HOUR']).size().reset_index(name='Count')
          day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
          temporal_data['DAY_OF_WEEK'] = pd.Categorical(temporal_data['DAY_OF_WEEK'], categories=day_order, ordered=True)
          temporal_data = temporal_data.sort_values(['DAY_OF_WEEK', 'HOUR'])

          fig_temporal = px.density_heatmap(temporal_data, x='HOUR', y='DAY_OF_WEEK', z='Count',
                                           color_continuous_scale='viridis')
          fig_temporal.update_layout(template=pink_template)
     else:
          fig_temporal = go.Figure()
          fig_temporal.update_layout(title="No temporal data available")

     # 4. Severity Factors
     if 'SEVERITY_SCORE' in dff.columns:
          severity_factors = dff.groupby('BOROUGH')['SEVERITY_SCORE'].mean().reset_index()
          severity_factors = severity_factors.sort_values('SEVERITY_SCORE', ascending=False)

          fig_severity = px.bar(severity_factors, x='BOROUGH', y='SEVERITY_SCORE',
                               color='BOROUGH', color_discrete_map=BOROUGH_COLORS)
          fig_severity.update_layout(template=pink_template, showlegend=False)
     else:
          fig_severity = go.Figure()
          fig_severity.update_layout(title="No severity data available")

     # 5. Risk Density Map
     df_risk = dff.dropna(subset=["LATITUDE", "LONGITUDE"]).copy()
     if not df_risk.empty:
          # sample for density plotting to limit rendering time
          if len(df_risk) > MAX_DENSITY_POINTS:
               df_risk_plot = df_risk.sample(n=MAX_DENSITY_POINTS, random_state=42)
          else:
               df_risk_plot = df_risk
          fig_density = px.density_mapbox(df_risk_plot, lat='LATITUDE', lon='LONGITUDE',
                                         z='SEVERITY_SCORE', radius=20,
                                         zoom=9, height=500,
                                         mapbox_style="open-street-map",
                                         color_continuous_scale="viridis")
          fig_density.update_layout(template=pink_template)
     else:
          fig_density = go.Figure()
          fig_density.update_layout(title="No location data for density map")

     # Live statistics as a simple dict (Streamlit will render metrics separately)
     live_stats = {
          "total_crashes": int(total_crashes),
          "total_injuries": int(total_injuries),
          "total_fatalities": int(total_killed),
          "avg_injuries_per_crash": float(avg_injuries_per_crash)
     }

     # Return a dict of named figures + stats for easier Streamlit consumption
     return {
          # LAMA'S FIGURES
          "fig_inj_borough": fig_inj_borough,
          "fig_factor": fig_factor,
          "fig_year": fig_year,
          "fig_map": fig_map,
          "fig_gender": fig_gender,
          "fig_safety": fig_safety,
          "fig_emotional": fig_emotional,
          "fig_age_hist": fig_age_hist,
          "fig_person_time": fig_person_time,
          "summary": summary,
          # Additional Figures
          "fig_zip": fig_zip,
          "fig_top_streets": fig_top_streets,
          "fig_top_intersections": fig_top_intersections,
          "fig_vehicle_cooccurrence": fig_vehicle_cooccurrence,
          "fig_injury_counts": fig_injury_counts,
          "fig_factor2": fig_factor2,
          "fig_factor3": fig_factor3,
          "fig_ped_role": fig_ped_role,
          "fig_daily_trend": fig_daily_trend,

          # DAREEN'S FIGURES
          "fig_borough_dareen": fig_borough_dareen,
          "fig_injury_dareen": fig_injury_dareen,
          "fig_ejection": fig_ejection,
          "fig_complaint": fig_complaint,
          "fig_vehicle_factor": fig_vehicle_factor,
          "fig_position": fig_position,
          "fig_vehicle_trend": fig_vehicle_trend,

          # ADVANCED ANALYTICS
          "fig_hotspot": fig_hotspot,
          "fig_corr": fig_corr,
          "fig_temporal": fig_temporal,
          "fig_severity": fig_severity,
          "fig_density": fig_density,

          # Live Stats
          "live_stats": live_stats,
     }

def get_default_filters():
     min_y = int(df["YEAR"].min()) if not df["YEAR"].isna().all() else 2010
     max_y = int(df["YEAR"].max()) if not df["YEAR"].isna().all() else pd.Timestamp.now().year

     return (
          [min_y, max_y],  # year_slider
          None,  # borough_filter
          None,  # vehicle_filter
          None,  # factor_filter
          None,  # injury_filter
          None,  # person_type_filter
          ""     # search_input
     )


# If this file is executed directly, try to run the Dash server.
# If run via `streamlit run`, Dash may not be able to start; in that
# case we fall back to rendering a subset of the figures using Streamlit
# so users running `streamlit run` still see charts.
if __name__ == "__main__":
     st.write("## NYC Crash Dashboard â€” Streamlit Mode")

     # Sidebar controls
     with st.sidebar:
          st.header("Filters")
          # Load dataset on demand to avoid import-time crashes on Streamlit Cloud
          if st.button("Load dataset"):
               _df = load_data()
               if not _df.empty:
                    # store loaded DataFrame into module-global `df` and rerun
                    globals()['df'] = _df
                    st.experimental_rerun()

          if df.empty:
               st.warning("Dataset not loaded. Click 'Load dataset' to load the CSV/parquet (may be large).")
               # Provide defaults so UI renders
               default_filters = get_default_filters()
               year_slider = st.slider("Year Range", min_value=int(default_filters[0][0]), max_value=int(default_filters[0][1]), value=(int(default_filters[0][0]), int(default_filters[0][1])))
               boroughs_sel = None
               vehicle_sel = None
               factor_sel = None
               person_type_sel = None
               injury_sel = None
               search_text = ""
               clear_btn = False
          else:
               year_slider = st.slider(
                    "Year Range",
                    min_value=int(min_year),
                    max_value=int(max_year),
                    value=(int(min_year), int(max_year)),
               )
               boroughs_sel = st.multiselect("Borough", options=sorted(df["BOROUGH"].dropna().unique()), default=None)
               vehicle_sel = st.multiselect("Vehicle Type", options=sorted({vt for sub in df.get("VEHICLE_TYPES_LIST", []) for vt in sub}), default=None)
               factor_sel = st.multiselect("Contributing Factor", options=sorted({f for sub in df.get("FACTORS_LIST", []) for f in sub}), default=None)
               person_type_sel = st.multiselect("Person Type", options=sorted(df.get("PERSON_TYPE", pd.Series([])).dropna().unique()), default=None)
               injury_sel = st.multiselect("Injury Type", options=sorted(df.get("PERSON_INJURY", pd.Series([])).dropna().unique()), default=None)
               search_text = st.text_input("Advanced Search", value="")
               clear_btn = st.button("Clear Filters")

     # Manage clear filters
     if clear_btn:
          boroughs_sel = None
          vehicle_sel = None
          factor_sel = None
          person_type_sel = None
          injury_sel = None
          search_text = ""
          year_slider = (int(min_year), int(max_year))
          # (automatic update mode) clear resets filters; visuals update automatically

     # Compute figures immediately (automatic update mode)
     outputs = None
     try:
          with st.spinner("Generating charts â€” this may take a few seconds..."):
               outputs = compute_figures(list(year_slider), boroughs_sel, vehicle_sel, factor_sel, injury_sel, person_type_sel, search_text)
     except Exception as e:
          st.error(f"Could not compute dashboard figures: {e}")

     if outputs:
          # Read named figures and stats from returned dict
          fig_inj_borough = outputs.get("fig_inj_borough")
          fig_factor = outputs.get("fig_factor")
          fig_year = outputs.get("fig_year")
          fig_map = outputs.get("fig_map")
          fig_gender = outputs.get("fig_gender")
          fig_safety = outputs.get("fig_safety")
          fig_emotional = outputs.get("fig_emotional")
          fig_age_hist = outputs.get("fig_age_hist")
          fig_person_time = outputs.get("fig_person_time")
          summary = outputs.get("summary", "")

          fig_borough_dareen = outputs.get("fig_borough_dareen")
          fig_injury_dareen = outputs.get("fig_injury_dareen")
          fig_ejection = outputs.get("fig_ejection")
          fig_complaint = outputs.get("fig_complaint")
          fig_vehicle_factor = outputs.get("fig_vehicle_factor")
          fig_position = outputs.get("fig_position")
          fig_vehicle_trend = outputs.get("fig_vehicle_trend")

          fig_hotspot = outputs.get("fig_hotspot")
          fig_corr = outputs.get("fig_corr")
          fig_temporal = outputs.get("fig_temporal")
          fig_severity = outputs.get("fig_severity")
          fig_density = outputs.get("fig_density")

          # Additional figures
          fig_zip = outputs.get("fig_zip")
          fig_top_streets = outputs.get("fig_top_streets")
          fig_top_intersections = outputs.get("fig_top_intersections")
          fig_vehicle_cooccurrence = outputs.get("fig_vehicle_cooccurrence")
          fig_injury_counts = outputs.get("fig_injury_counts")
          fig_factor2 = outputs.get("fig_factor2")
          fig_factor3 = outputs.get("fig_factor3")
          fig_ped_role = outputs.get("fig_ped_role")
          fig_daily_trend = outputs.get("fig_daily_trend")

          live_stats = outputs.get("live_stats", {})

          # Parse summary for live-stat numbers
          totals = {"crashes": 0, "injured": 0, "fatalities": 0}
          try:
               parts = summary.split(':', 1)[1]
               parts = [p.strip() for p in parts.split('|')]
               if len(parts) >= 3:
                    totals['crashes'] = int(parts[0].split()[0].replace(',', ''))
                    totals['injured'] = int(parts[1].split()[0].replace(',', ''))
                    totals['fatalities'] = int(parts[2].split()[0].replace(',', ''))
          except Exception:
               pass

          # Top header
          st.markdown(f"**{summary}**")

          tabs = st.tabs(["Crash Geography", "Vehicles & Factors", "People & Injuries", "Demographics", "Analytics"])

          # Tab 1: Crash Geography
          with tabs[0]:
               st.subheader("Crash Locations Map")
               st.plotly_chart(fig_map, use_container_width=True)
               st.subheader("Crashes Over Time")
               st.plotly_chart(fig_year, use_container_width=True)
               st.subheader("Crashes by Borough")
               st.plotly_chart(fig_borough_dareen, use_container_width=True)
               st.subheader("Injuries by Borough")
               st.plotly_chart(fig_inj_borough, use_container_width=True)
               st.subheader("Top ZIP Codes")
               st.plotly_chart(fig_zip, use_container_width=True)
               st.subheader("Top Streets")
               st.plotly_chart(fig_top_streets, use_container_width=True)
               st.subheader("Top Intersections")
               st.plotly_chart(fig_top_intersections, use_container_width=True)
               st.subheader("Daily Crash Trend")
               st.plotly_chart(fig_daily_trend, use_container_width=True)

          # Tab 2: Vehicles & Factors
          with tabs[1]:
               st.subheader("Contributing Factors")
               st.plotly_chart(fig_factor, use_container_width=True)
               st.subheader("Vehicle vs Factor Heatmap")
               st.plotly_chart(fig_vehicle_factor, use_container_width=True)
               st.subheader("Vehicle Trends")
               st.plotly_chart(fig_vehicle_trend, use_container_width=True)
               st.subheader("Vehicle Co-occurrence")
               st.plotly_chart(fig_vehicle_cooccurrence, use_container_width=True)
               st.subheader("Contributing Factor 2")
               st.plotly_chart(fig_factor2, use_container_width=True)
               st.subheader("Contributing Factor 3")
               st.plotly_chart(fig_factor3, use_container_width=True)

          # Tab 3: People & Injuries
          with tabs[2]:
               st.subheader("Safety Equipment")
               st.plotly_chart(fig_safety, use_container_width=True)
               st.subheader("Injury Types")
               st.plotly_chart(fig_injury_dareen, use_container_width=True)
               st.subheader("Emotional State")
               st.plotly_chart(fig_emotional, use_container_width=True)
               st.subheader("Position in Vehicle")
               st.plotly_chart(fig_position, use_container_width=True)
               st.subheader("Injury Counts Breakdown")
               st.plotly_chart(fig_injury_counts, use_container_width=True)
               st.subheader("Pedestrian Roles")
               st.plotly_chart(fig_ped_role, use_container_width=True)

          # Tab 4: Demographics
          with tabs[3]:
               st.subheader("Age Distribution")
               st.plotly_chart(fig_age_hist, use_container_width=True)
               st.subheader("Gender Distribution")
               st.plotly_chart(fig_gender, use_container_width=True)

          # Tab 5: Advanced Analytics
          with tabs[4]:
               st.subheader("Hotspot Clusters")
               st.plotly_chart(fig_hotspot, use_container_width=True)
               st.subheader("Correlation Heatmap")
               st.plotly_chart(fig_corr, use_container_width=True)
               st.subheader("Temporal Patterns")
               st.plotly_chart(fig_temporal, use_container_width=True)
               st.subheader("Severity by Borough")
               st.plotly_chart(fig_severity, use_container_width=True)
               st.subheader("Risk Density Map")
               st.plotly_chart(fig_density, use_container_width=True)

          # Live stats as columns
          st.markdown("---")
          c1, c2, c3 = st.columns(3)
          c1.metric("Total Crashes", f"{totals['crashes']:,}")
          c2.metric("Total Injuries", f"{totals['injured']:,}")
          c3.metric("Total Fatalities", f"{totals['fatalities']:,}")

          



